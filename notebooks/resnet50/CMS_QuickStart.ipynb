{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Hardware Accelerated Models Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show you how to deploy an image recognition service based on the ResNet 50 classifier in just a few minutes using the Azure Machine Learning Accelerated AI service.  Get more help from our [documentation](https://aka.ms/aml-real-time-ai) or [forum](https://aka.ms/aml-forum).\n",
    "\n",
    "We will use an accelerated ResNet50 featurizer running on an FPGA. This functionality is powered by Project Brainwave, which handles translating deep neural networks (DNN) into an FPGA program.\n",
    "\n",
    "## Request Quota\n",
    "**IMPORTANT:** You must [request quota](https://aka.ms/aml-real-time-ai-request) and be approved before you can successfully run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "1. Download and install [Git](https://git-scm.com/downloads) 2.16 or later\n",
    "1. Open a Git prompt and clone this repo:\n",
    "\n",
    "   `git clone https://github.com/Azure/aml-real-time-ai`\n",
    "1. Install conda (Python 3.6):\n",
    "\n",
    "   https://conda.io/miniconda.html\n",
    "1. Open an Anaconda Prompt and run the rest of the commands in the prompt. On Windows the prompt will look like:\n",
    "\n",
    "   `(base) C:\\>`\n",
    "1. Create the environment:\n",
    "\n",
    "   `conda env create -f aml-real-time-ai/environment.yml`\n",
    "1. Activate the environment:\n",
    "   1. Windows:\n",
    "   `conda activate amlrealtimeai`\n",
    "   1. Mac/Linux:\n",
    "   `source activate amlrealtimeai`\n",
    "1. Launch the Jupyter notebook browser:\n",
    "\n",
    "   `jupyter notebook` \n",
    "1. In the browser, open this notebook by navigating to notebooks/resnet50/00_QuickStart.ipynb.  (If you're using Chrome, copy and paste the URL with the notebook token into the address bar).\n",
    "\n",
    "1. Run through each cell and enter the appropriate information as necessary (e.g. Azure subscription ID, resource group ID, Model Management Account, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Management Account\n",
    "\n",
    "You can check the status of your accounts at with your Fermilab service account at portal.azure.com\n",
    "\n",
    "Glossary:\n",
    "   - Model = the ML model\n",
    "   - Service = the web service to access the ML model\n",
    "   - Model Management Account = manages models/services, one account as 50 (?) models and 1 service.\n",
    "\n",
    "Basic steps in this notebook:\n",
    "   - image preprocessing, import resnet50 as a featurizer and classifier\n",
    "   - define the service\n",
    "   - deploy the model\n",
    "   - deploy the service\n",
    "   - define client and do prediction\n",
    "   - cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import amlrealtimeai\n",
    "from amlrealtimeai import resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing\n",
    "We'd like our service to accept JPEG images as input. However the input to ResNet50 is a tensor. So we need code that decodes JPEG images and does the preprocessing required by ResNet50. The Accelerated AI service can execute TensorFlow graphs as part of the service and we'll use that ability to do the image preprocessing. This code defines a TensorFlow graph that preprocesses an array of JPEG images (as strings) and produces a tensor that is ready to be featurized by ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Input images as a two-dimensional tensor containing an arbitrary number of images represented a strings\n",
    "import amlrealtimeai.resnet50.utils\n",
    "in_images = tf.placeholder(tf.string)\n",
    "image_tensors = resnet50.utils.preprocess_array(in_images)\n",
    "print(image_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizer\n",
    "We use ResNet50 as a featurizer. In this step we initialize the model. This downloads a TensorFlow checkpoint of the quantized ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.6-rc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from amlrealtimeai.resnet50.model import LocalQuantizedResNet50\n",
    "model_path = os.path.expanduser('~/models')\n",
    "model = LocalQuantizedResNet50(model_path)\n",
    "print(model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "The model we downloaded includes a classifier which takes the output of the ResNet50 and identifies an image. This classifier is trained on the ImageNet dataset. We are going to use this classifier for our service. The next [notebook](01_ModelBuild.ipynb) shows how to train a classifier for a different data set. The input to the classifier is a tensor matching the output of our ResNet50 featurizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 1, 2048)\n"
     ]
    }
   ],
   "source": [
    "model.import_graph_def(include_featurizer=False)\n",
    "print(model.classifier_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Definition\n",
    "Now that we've definied the image preprocessing, featurizer, and classifier that we will execute on our service we can create a service definition. The service definition is a set of files generated from the model that allow us to deploy to the FPGA service. The service definition consists of a pipeline. The pipeline is a series of stages that are executed in order. We support TensorFlow stages, Keras stages, and BrainWave stages. The stages will be executed in order on the service, with the output of each stage input into the subsequent stage.\n",
    "\n",
    "To create a TensorFlow stage we specify a session containing the graph (in this case we are using the default graph) and the input and output tensors to this stage. We use this information to save the graph so that we can execute it on the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n",
      "INFO:tensorflow:Froze 0 variables.\n",
      "Converted 0 variables to const ops.\n",
      "/Users/ntran/models/save/service_def.zip\n"
     ]
    }
   ],
   "source": [
    "from amlrealtimeai.pipeline import ServiceDefinition, TensorflowStage, BrainWaveStage\n",
    "\n",
    "save_path = os.path.expanduser('~/models/save')\n",
    "service_def_path = os.path.join(save_path, 'service_def.zip')\n",
    "\n",
    "service_def = ServiceDefinition()\n",
    "service_def.pipeline.append(TensorflowStage(tf.Session(), in_images, image_tensors))\n",
    "service_def.pipeline.append(BrainWaveStage(model))\n",
    "service_def.pipeline.append(TensorflowStage(tf.Session(), model.classifier_input, model.classifier_output))\n",
    "service_def.save(service_def_path)\n",
    "print(service_def_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "Time to create a service from the service definition. You need a Model Management Account in the **East US 2** location. Go to our [GitHub repo](https://aka.ms/aml-real-time-ai) \"docs\" folder to learn how to create a Model Management Account and find the required information below.\n",
    "\n",
    "This code creates the deployment client that we will use to deploy the service. Follow the instructions in the output to sign in to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code EW8Y3B9WV to authenticate.\n"
     ]
    }
   ],
   "source": [
    "from amlrealtimeai import DeploymentClient\n",
    "\n",
    "subscription_id = \"80defacd-509e-410c-9812-6e52ed6a0016\"\n",
    "resource_group = \"CMS_FPGA_Resources\"\n",
    "model_management_account = \"CMS_FPGA_1\"\n",
    "\n",
    "model_name = \"resnet50-model-nvt\"\n",
    "service_name = \"quickstart-service\"\n",
    "\n",
    "deployment_client = DeploymentClient(subscription_id, resource_group, model_management_account)\n",
    "\n",
    "useExistingModelAndService = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the service definition to the model management service. <br>\n",
    "*n.b. if you're not uploading a new model you don't have to do the next line*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model resnet50-model-nvt\n",
      "Successfully registered model resnet50-model-nvt\n"
     ]
    }
   ],
   "source": [
    "model_id = deployment_client.register_model(model_name, service_def_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a service from the model that we registered. If this is a new service then we create it. If you already have a service with this name then the existing service will be updated to use this model. <br>\n",
    "*n.b. this takes a few minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = deployment_client.get_service_by_name(service_name)\n",
    "if not useExistingModelAndService:\n",
    "    if(service is None):\n",
    "        service = deployment_client.create_service(service_name, model_id)    \n",
    "    else:\n",
    "        service = deployment_client.update_service(service.id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "The service supports gRPC and the TensorFlow Serving \"predict\" API. We provide a client that can call the service to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amlrealtimeai import PredictionClient\n",
    "client = PredictionClient(service.ipAddress, service.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the results we need a mapping to the human readable imagenet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "classes_entries = requests.get(\"https://raw.githubusercontent.com/Lasagne/Recipes/master/examples/resnet50/imagenet_classes.txt\").text.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send an image to the service and get the predictions. Let's see if it can identify a snow leopard.\n",
    "![title](snowleopardgaze.jpg)\n",
    "Snow leopard in a zoo. Photo by Peter Bolliger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow leopard, ounce, Panthera uncia confidence: 0.9664236\n",
      "leopard, Panthera pardus confidence: 0.03326854\n",
      "lynx, catamount confidence: 0.00012824342\n",
      "cheetah, chetah, Acinonyx jubatus confidence: 6.9350055e-05\n",
      "jaguar, panther, Panthera onca, Felis onca confidence: 6.211184e-05\n"
     ]
    }
   ],
   "source": [
    "image_file = 'snowleopardgaze.jpg'\n",
    "results = client.score_image(image_file)\n",
    "# map results [class_id] => [confidence]\n",
    "results = enumerate(results)\n",
    "# sort results by confidence\n",
    "sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "# print top 5 results\n",
    "for top in sorted_results[:5]:\n",
    "    print(classes_entries[top[0]], 'confidence:', top[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_QuickStart.ipynb  CMS_QuickStart.ipynb snowleopardgaze.jpg\n",
      "01_ModelBuild.ipynb  README.md\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1348k  100 1348k    0     0  4218k      0 --:--:-- --:--:-- --:--:-- 4214k\n",
      "00_QuickStart.ipynb        CMS_QuickStart.ipynb\n",
      "01_ModelBuild.ipynb        README.md\n",
      "American_bison_k5680-1.jpg snowleopardgaze.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!curl -O https://upload.wikimedia.org/wikipedia/commons/8/8d/American_bison_k5680-1.jpg\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a bison now!\n",
    "![title](American_bison_k5680-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bison confidence: 0.99977654\n",
      "water buffalo, water ox, Asiatic buffalo, Bubalus bubalis confidence: 7.485624e-05\n",
      "wild boar, boar, Sus scrofa confidence: 6.220631e-05\n",
      "ram, tup confidence: 2.6434049e-05\n",
      "warthog confidence: 2.0652597e-05\n"
     ]
    }
   ],
   "source": [
    "image_file = 'American_bison_k5680-1.jpg'\n",
    "results = client.score_image(image_file)\n",
    "# map results [class_id] => [confidence]\n",
    "results = enumerate(results)\n",
    "# sort results by confidence\n",
    "sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "# print top 5 results\n",
    "for top in sorted_results[:5]:\n",
    "    print(classes_entries[top[0]], 'confidence:', top[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a jet now!\n",
    "![title](jet_image.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analog clock , confidence: 0.8578913\n",
      "candle, taper, wax light , confidence: 0.059348024\n",
      "whistle , confidence: 0.02801542\n",
      "torch , confidence: 0.018430172\n",
      "digital clock , confidence: 0.007941563\n"
     ]
    }
   ],
   "source": [
    "image_file = 'jet_image.jpg'\n",
    "results = client.score_image(image_file)\n",
    "# map results [class_id] => [confidence]\n",
    "results = enumerate(results)\n",
    "# sort results by confidence\n",
    "sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "# print top 5 results\n",
    "for top in sorted_results[:5]:\n",
    "    print(classes_entries[top[0]], ', confidence:', top[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Run the cell below to delete your service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = deployment_client.list_services()\n",
    "\n",
    "for service in filter(lambda x: x.name == service_name, services):\n",
    "    print(service.id)\n",
    "    deployment_client.delete_service(service.id)\n",
    "    \n",
    "models = deployment_client.list_models()\n",
    "\n",
    "for model in filter(lambda x: x.name == model_name, models):\n",
    "    print(model.id)\n",
    "    deployment_client.delete_model(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've just created a service that does predictions using an FPGA. The next [notebook](01_ModelBuild.ipynb) shows how to customize the service using transfer learning to classify different types of images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
